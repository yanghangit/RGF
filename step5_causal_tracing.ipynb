{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkVWRFM44KhK"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"reverse-gene-finder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hInH2FPWWalw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "PROJECT_HOME = os.path.join(\"/content/drive/My Drive/Projects\", PROJECT_NAME)\n",
        "\n",
        "import sys\n",
        "sys.path.append(PROJECT_HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJYOEgFb4KhM"
      },
      "outputs": [],
      "source": [
        "# Google Drive storage setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giYhcHcVrgBG"
      },
      "outputs": [],
      "source": [
        "%pip install -U tdigest anndata scanpy loompy > /dev/null 2> /dev/null\n",
        "%pip install -U transformers[torch] datasets > /dev/null 2> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59nXleT8ZFCK"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.auto import trange\n",
        "from collections import defaultdict\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "from datasets import load_from_disk\n",
        "\n",
        "from libs.classifier import Classifier\n",
        "from libs.causal_trace import trace_important_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLXMbyOvT-BI"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdHp1sqzgQcu"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/ctheodoris/Geneformer (Apache License 2.0)\n",
        "\n",
        "def pad_tensor(tensor, pad_token_id, max_len):\n",
        "    tensor = torch.nn.functional.pad(\n",
        "        tensor, pad=(0, max_len - tensor.numel()), mode=\"constant\", value=pad_token_id\n",
        "    )\n",
        "\n",
        "    return tensor\n",
        "\n",
        "def pad_2d_tensor(tensor, pad_token_id, max_len, dim):\n",
        "    if dim == 0:\n",
        "        pad = (0, 0, 0, max_len - tensor.size()[dim])\n",
        "    elif dim == 1:\n",
        "        pad = (0, max_len - tensor.size()[dim], 0, 0)\n",
        "    tensor = torch.nn.functional.pad(\n",
        "        tensor, pad=pad, mode=\"constant\", value=pad_token_id\n",
        "    )\n",
        "    return tensor\n",
        "\n",
        "def pad_3d_tensor(tensor, pad_token_id, max_len, dim):\n",
        "    if dim == 0:\n",
        "        raise Exception(\"dim 0 usually does not need to be padded.\")\n",
        "    if dim == 1:\n",
        "        pad = (0, 0, 0, max_len - tensor.size()[dim])\n",
        "    elif dim == 2:\n",
        "        pad = (0, max_len - tensor.size()[dim], 0, 0)\n",
        "    tensor = torch.nn.functional.pad(\n",
        "        tensor, pad=pad, mode=\"constant\", value=pad_token_id\n",
        "    )\n",
        "    return tensor\n",
        "\n",
        "# pad list of tensors and convert to tensor\n",
        "def pad_tensor_list(\n",
        "    tensor_list,\n",
        "    dynamic_or_constant,\n",
        "    pad_token_id,\n",
        "    model_input_size,\n",
        "    dim=None,\n",
        "    padding_func=None,\n",
        "):\n",
        "    # determine maximum tensor length\n",
        "    if dynamic_or_constant == \"dynamic\":\n",
        "        max_len = max([tensor.squeeze().numel() for tensor in tensor_list])\n",
        "    elif isinstance(dynamic_or_constant, int):\n",
        "        max_len = dynamic_or_constant\n",
        "    else:\n",
        "        max_len = model_input_size\n",
        "\n",
        "    # pad all tensors to maximum length\n",
        "    if dim is None:\n",
        "        tensor_list = [\n",
        "            pad_tensor(tensor, pad_token_id, max_len) for tensor in tensor_list\n",
        "        ]\n",
        "    else:\n",
        "        tensor_list = [\n",
        "            padding_func(tensor, pad_token_id, max_len, dim) for tensor in tensor_list\n",
        "        ]\n",
        "    # return stacked tensors\n",
        "    if padding_func != pad_3d_tensor:\n",
        "        return torch.stack(tensor_list)\n",
        "    else:\n",
        "        return torch.cat(tensor_list, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILUOm9wxyeJQ"
      },
      "outputs": [],
      "source": [
        "# Load gene token dictionary\n",
        "\n",
        "token_dictionary_file = os.path.join(PROJECT_HOME, \"libs\", \"token_dictionary.pkl\")\n",
        "with open(token_dictionary_file, \"rb\") as f:\n",
        "    gene_token_dict = pickle.load(f)\n",
        "\n",
        "token_gene_dict = {v: k for k, v in gene_token_dict.items()}\n",
        "pad_token_id = gene_token_dict.get(\"<pad>\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load gene information\n",
        "\n",
        "gene_info = pd.read_csv(os.path.join(PROJECT_HOME, \"data\", \"gene_info.csv\"))\n",
        "gene_id_to_name = {}\n",
        "gene_name_to_id = {}\n",
        "for idx, row in gene_info.iterrows():\n",
        "    gene_id_to_name[row['gene_id']] = row['gene_name']\n",
        "    gene_name_to_id[row['gene_name']] = row['gene_id']"
      ],
      "metadata": {
        "id": "X1i6uHROID2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svMzE132gQcv"
      },
      "outputs": [],
      "source": [
        "# Known AD-related genes\n",
        "\n",
        "# http://www.alzgene.org/TopResults.asp\n",
        "selected_genes = [\"APOE\", \"BIN1\", \"CLU\", \"ABCA7\", \"CR1\", \"PICALM\", \"MS4A6A\", \"CD33\", \"MS4A4E\", \"CD2AP\"]\n",
        "\n",
        "selected_token_ids = []\n",
        "for gene_name in selected_genes:\n",
        "    if gene_name in gene_name_to_id:\n",
        "        gene_id = gene_name_to_id[gene_name]\n",
        "        selected_token_ids.append(gene_token_dict.get(gene_id))\n",
        "selected_token_ids = set(selected_token_ids)\n",
        "\n",
        "print(\"# of known AD genes: %d\" % len(selected_token_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpmjy2XONGPf"
      },
      "outputs": [],
      "source": [
        "# Causal tracing settings\n",
        "\n",
        "max_seq_len = 256 # maximum sequence length\n",
        "n_samples = 10 # 1 original sample plus 9 corrupted samples\n",
        "noise = 1.0 # noise level when perturbing the input sample\n",
        "\n",
        "# Identify the effects of input perturbations on the model's output\n",
        "\n",
        "indirect_effects_list = []\n",
        "attention_weights_list = []\n",
        "input_ids_list = []\n",
        "\n",
        "for CV_FOLD in range(5):\n",
        "\n",
        "    model_dir = os.path.join(PROJECT_HOME, \"models\", \"finetuned_models\", \"cv_%d\" % CV_FOLD)\n",
        "    model_prefix = \"ad_cell_classifier\"\n",
        "    model_directory = f\"{model_dir}/geneformer_cellClassifier_{model_prefix}/ksplit1/\"\n",
        "    test_data_file = f\"{model_dir}/{model_prefix}_labeled_test.dataset\"\n",
        "\n",
        "    input_data = load_from_disk(test_data_file)\n",
        "\n",
        "    num_layers = 12\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        model_directory,\n",
        "        num_labels=len(['nonAD', 'earlyAD']),\n",
        "        output_hidden_states=True,\n",
        "        output_attentions=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    total_batch_length = len(input_data)\n",
        "    forward_batch_size = 1\n",
        "\n",
        "    for i in trange(0, total_batch_length, forward_batch_size, leave=True, desc=\"CV Fold %s\" % CV_FOLD):\n",
        "\n",
        "        max_range = min(i + forward_batch_size, total_batch_length)\n",
        "        minibatch = input_data.select([i for i in range(i, max_range)])\n",
        "        minibatch.set_format(type=\"torch\")\n",
        "\n",
        "        input_data_minibatch = minibatch[\"input_ids\"]\n",
        "        input_data_minibatch = input_data_minibatch.to(\"cuda\")\n",
        "        input_ids = input_data_minibatch[0]\n",
        "        if len(input_ids) > max_seq_len:\n",
        "            # keep the high-rank and low-rank genes\n",
        "            num_samples_one_side = int(max_seq_len/2)\n",
        "            input_ids = torch.cat((input_ids[:num_samples_one_side], input_ids[-num_samples_one_side:]))\n",
        "            input_data_minibatch = torch.stack([input_ids])\n",
        "\n",
        "        input_data_minibatch = pad_tensor_list(\n",
        "            input_data_minibatch, max_seq_len, pad_token_id, forward_batch_size\n",
        "        )\n",
        "\n",
        "        input_ids = input_data_minibatch[0].detach().cpu()\n",
        "        e_range = []\n",
        "        for token_idx, token_id in enumerate(input_ids):\n",
        "            token_id = int(token_id)\n",
        "            if token_id in selected_token_ids:\n",
        "                e_range.append(token_idx)\n",
        "        if len(e_range) == 0:\n",
        "            continue # Skip sample with no known AD genes\n",
        "        input_ids_list.append(input_ids)\n",
        "\n",
        "        attention_weights = model(input_data_minibatch).attentions\n",
        "        average_attention_weights_list = []\n",
        "        for layer in range(num_layers):\n",
        "            # Average attention weights across all heads\n",
        "            # Shape: (seq_length, seq_length)\n",
        "            average_attention_weights_at_one_layer = attention_weights[layer].mean(dim=1).squeeze(0)\n",
        "            average_attention_weights_list.append(average_attention_weights_at_one_layer)\n",
        "        average_attention_weights = torch.stack(average_attention_weights_list)\n",
        "        average_attention_weights = average_attention_weights.detach().cpu()\n",
        "\n",
        "        # Shape: (seq_length, num_layers)\n",
        "        inp = torch.cat([input_data_minibatch[:] for _ in range(n_samples)])\n",
        "        indirect_effects = trace_important_states(model, num_layers, inp, e_range=e_range, noise=noise)\n",
        "        indirect_effects = indirect_effects.detach().cpu()\n",
        "\n",
        "        indirect_effects_list.append(indirect_effects)\n",
        "        attention_weights_list.append(average_attention_weights)\n",
        "\n",
        "all_indirect_effects = torch.stack(indirect_effects_list)\n",
        "all_attention_weights = torch.stack(attention_weights_list)\n",
        "all_input_ids = torch.stack(input_ids_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A8l3ae3kuRO"
      },
      "outputs": [],
      "source": [
        "np.save(os.path.join(PROJECT_HOME, \"results\", \"indirect_effects.npy\"), all_indirect_effects.numpy())\n",
        "np.save(os.path.join(PROJECT_HOME, \"results\", \"attention_weights.npy\"), all_attention_weights.numpy())\n",
        "np.save(os.path.join(PROJECT_HOME, \"results\", \"input_ids.npy\"), all_input_ids.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "uWgwxPogMXaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}